{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08667a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import nltk \n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f07e9551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Airtel_Pos</th>\n",
       "      <th>Airtel_Neut</th>\n",
       "      <th>Airtel_Neg</th>\n",
       "      <th>Saf_Pos</th>\n",
       "      <th>Saf_Neut</th>\n",
       "      <th>Saf_Neg</th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@emukala85 Apologies for the inconveniences ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>apologies inconveniences caused notified recha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Communications Authority: Airtel Kenya had the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>communications authority airtel kenya worst mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  Airtel_Pos  Airtel_Neut  \\\n",
       "0  @emukala85 Apologies for the inconveniences ca...           0            1   \n",
       "1  Communications Authority: Airtel Kenya had the...           0            0   \n",
       "\n",
       "   Airtel_Neg  Saf_Pos  Saf_Neut  Saf_Neg  \\\n",
       "0           0        0         0        1   \n",
       "1           1        0         1        0   \n",
       "\n",
       "                                        clean_tweets  \n",
       "0  apologies inconveniences caused notified recha...  \n",
       "1  communications authority airtel kenya worst mo...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c281974c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3274"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e74546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CountVectorizer(), TfidfVectorizer())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv, tfidf = CountVectorizer(), TfidfVectorizer()\n",
    "cv.fit(train_df['clean_tweets']), tfidf.fit(train_df['clean_tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "449a007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['clean_tweets']\n",
    "y = train_df.drop(columns=[\"tweet\", \"clean_tweets\"], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "913d92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv, X_tfidf = cv.transform(X), tfidf.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4df6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv, y_train, y_test = train_test_split(X_cv, y, test_size=0.3, random_state=2022)\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cad48fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OneVsRestClassifier(estimator=LogisticRegression()),\n",
       " OneVsRestClassifier(estimator=DecisionTreeClassifier()),\n",
       " OneVsRestClassifier(estimator=RandomForestClassifier()),\n",
       " OneVsRestClassifier(estimator=SVC()),\n",
       " OneVsRestClassifier(estimator=MLPClassifier())]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(), SVC(), MLPClassifier()]\n",
    "ovr_models = list()\n",
    "for model in models:\n",
    "    ovr_model = OneVsRestClassifier(model)\n",
    "    ovr_models.append(ovr_model)\n",
    "ovr_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4398ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting on the two sets of vectors\n",
    "model_accs_cv = []\n",
    "model_f1s_cv = []\n",
    "model_names = [\"logistic regression\", \"decision tree\", \"random forest\", \"support vector machine\", \"perceptron\"]\n",
    "for model in ovr_models:\n",
    "    model.fit(X_train_cv,y_train)\n",
    "    predictions = model.predict(X_test_cv)\n",
    "    f1_score_cv = f1_score(predictions, y_test, average=\"weighted\")\n",
    "    acc_score_cv = accuracy_score(predictions, y_test)\n",
    "    model_accs_cv.append(acc_score_cv), model_f1s_cv.append(f1_score_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf3b84",
   "metadata": {},
   "source": [
    "# Results\n",
    "{model: [model accuracy, model f1_score]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a4a2cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stand Alone Model Results: Count-vectorizer\n",
      "{'logistic regression': [0.2990844354018311, 0.6715337409742348], 'decision tree': [0.1770091556459817, 0.6149613965988247], 'random forest': [0.19328585961342828, 0.6373546468890064], 'support vector machine': [0.07324516785350967, 0.587587155516257], 'perceptron': [0.3896236012207528, 0.7209713366149341]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Stand Alone Model Results: Count-vectorizer\")\n",
    "cv_experiment_results = dict((z[0], list(z[1:])) for z in zip(model_names, model_accs_cv, model_f1s_cv)) \n",
    "print(cv_experiment_results)\n",
    "with open(\"count_vectorizer_ml_experiment_results_com.json\", \"w\") as outfile:\n",
    "    json.dump(cv_experiment_results, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30886c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting on the two sets of vectors\n",
    "model_accs_tfidf = []\n",
    "model_f1s_tfidf = []\n",
    "model_names = [\"logistic regression\", \"decision tree\", \"random forest\", \"support vector machine\", \"perceptron\"]\n",
    "for model in ovr_models:\n",
    "    model.fit(X_train_tfidf,y_train)\n",
    "    predictions = model.predict(X_test_tfidf)\n",
    "    f1_score_tfidf = f1_score(predictions, y_test, average=\"weighted\")\n",
    "    acc_score_tfidf = accuracy_score(predictions, y_test)\n",
    "    model_accs_tfidf.append(acc_score_tfidf), model_f1s_tfidf.append(f1_score_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60502542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stand Alone Model Results: TFIDF\n",
      "{'logistic regression': [0.07121057985757884, 0.5080021857786827], 'decision tree': [0.15666327568667346, 0.5917940883436114], 'random forest': [0.14954221770091555, 0.615736849166816], 'support vector machine': [0.07324516785350967, 0.5857961421958123], 'perceptron': [0.35300101729399797, 0.7077509465490691]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Stand Alone Model Results: TFIDF\")\n",
    "tfidf_experiment_results = dict((z[0], list(z[1:])) for z in zip(model_names, model_accs_tfidf, model_f1s_tfidf)) \n",
    "print(tfidf_experiment_results)\n",
    "with open(\"TFIDF_ml_experiment_results_com.json\", \"w\") as outfile:\n",
    "    json.dump(tfidf_experiment_results, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7667182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hybrid model\n",
    "base_estimators = [\n",
    "    (\"log reg\", OneVsRestClassifier(LogisticRegression())), \n",
    "    (\"decision tree\", OneVsRestClassifier(DecisionTreeClassifier())), \n",
    "    (\"random forest\", OneVsRestClassifier(RandomForestClassifier())), \n",
    "    (\"MLP\", OneVsRestClassifier(MLPClassifier()))\n",
    "]\n",
    "final_estimator = OneVsRestClassifier(StackingClassifier(estimators=base_estimators, final_estimator=OneVsRestClassifier(MLPClassifier())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45a5fbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Stacked cv results': [0.3957273652085453, 0.7158894610993464]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(cv_results)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacking_cv_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m--> 180\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "#FITTING AND TESTNIG ON CV VECTORS\n",
    "final_estimator.fit(X_train_cv, y_train)\n",
    "cv_hybrid_preds = final_estimator.predict(X_test_cv)\n",
    "cv_hybrid_acc = accuracy_score(y_test, cv_hybrid_preds)\n",
    "cv_hybrid_f1 = f1_score(y_test, cv_hybrid_preds, average=\"weighted\")\n",
    "cv_results = {\"Stacked cv results\": [cv_hybrid_acc, cv_hybrid_f1]}\n",
    "print(cv_results)\n",
    "#with open(\"stacking_cv_results.json\", \"wb\") as outfile:\n",
    "#    json.dump(cv_results, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23c3d441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Stacked TFIDF results': [0.35198372329603256, 0.6986402163730435]}\n"
     ]
    }
   ],
   "source": [
    "#FITTING AND TESTNIG ON CV VECTORS\n",
    "final_estimator.fit(X_train_tfidf, y_train)\n",
    "tfidf_hybrid_preds = final_estimator.predict(X_test_tfidf)\n",
    "tfidf_hybrid_acc = accuracy_score(y_test, tfidf_hybrid_preds)\n",
    "tfidf_hybrid_f1 = f1_score(y_test, tfidf_hybrid_preds, average=\"weighted\")\n",
    "tfidf_results = {\"Stacked TFIDF results\": [tfidf_hybrid_acc, tfidf_hybrid_f1]}\n",
    "print(tfidf_results)\n",
    "#with open(\"stacking_tfidf_results.json\", \"wb\") as outfile:\n",
    "#    json.dump(tfidf_results, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d0e5e",
   "metadata": {},
   "source": [
    "# Using CV and Conditional Probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "300d6d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(train_df, test_size=0.3, random_state=42)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58fe236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_est = [\n",
    "    (\"svm\", SVC()), \n",
    "    (\"lr\", LogisticRegression())\n",
    "]\n",
    "final_est = StackingClassifier(estimators=base_est, final_estimator=SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45be7da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "def pr(y_i, y):\n",
    "    p = X_train_cv[y == y_i].sum(0)\n",
    "    return (p+1) / ((y == y_i).sum()+1)\n",
    "\n",
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1, y) / pr(0, y))\n",
    "    m = final_est\n",
    "    x_nb = X_train_cv.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n",
    "\n",
    "preds = np.zeros((len(test_data), 6))\n",
    "sample_outputs = list()\n",
    "for i, j in enumerate(y.columns):\n",
    "    #print('fit', j)\n",
    "    m, r = get_mdl(y_train[j])\n",
    "    #com_model = joblib.dump(m, \"sample_nb_log_reg.sav\")\n",
    "    preds[:, i] = m.predict(X_test_cv.multiply(r))#[:, 1]\n",
    "    sample_text = \"While Safaricom's bundles can be considered to be fast and long lasting, Airtel's bundles are often cheap.\"\n",
    "    enc_text = tfidf.transform([sample_text])\n",
    "    sample_output = m.predict(enc_text.multiply(r))#[:, 1]\n",
    "    print(sample_output)\n",
    "    sample_outputs.append(sample_output)\n",
    "    # set_of_rs.append(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d29aa925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + SVC plus nb weights using cv results\n",
      "Accuracy:0.2624618514750763\n",
      "F1 Score: 0.6920179495588085\n"
     ]
    }
   ],
   "source": [
    "#Model performance:\n",
    "print(\"Logistic Regression + SVC plus nb weights using cv results\")\n",
    "print(\"Accuracy:\" +str(accuracy_score(preds, y_test)))\n",
    "print(\"F1 Score: \" + str(f1_score(preds, y_test, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f1f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
